#!/usr/bin/env python3
"""
ì˜ì–´ ë‹¨ì–´ì¥ ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ë¹„ë™ê¸° ê³ ì† ë²„ì „)
- ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ 10ë°° ì´ìƒ ë¹ ë¦„
- Free Dictionary API + MyMemory ë²ˆì—­ API
"""

import json
import asyncio
import aiohttp
import time
import os
from pathlib import Path
from typing import Optional
import re

# ì„¤ì •
DICTIONARY_API = "https://api.dictionaryapi.dev/api/v2/entries/en/"
MYMEMORY_API = "https://api.mymemory.translated.net/get"
CONCURRENT_REQUESTS = 30  # ë™ì‹œ ìš”ì²­ ìˆ˜
BATCH_SIZE = 100  # ì €ì¥ ë°°ì¹˜ í¬ê¸°

# SCOWL ë‹¨ì–´ íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ì ë‹¤ìš´ë¡œë“œ í´ë”)
SCOWL_PATH = Path(r"C:\Users\hooni\Downloads\scowl-2020.12.07\final")


def load_scowl_words(max_words: int = 10000) -> list[str]:
    """SCOWLì—ì„œ ì¼ë°˜ ë‹¨ì–´ë§Œ ë¡œë“œ (proper names, abbreviations ì œì™¸)"""
    words = set()
    
    # ì¼ë°˜ ë‹¨ì–´ íŒŒì¼ë“¤ë§Œ ì„ íƒ (american-words.XX)
    word_files = [
        "american-words.10",  # ê°€ì¥ í”í•œ ë‹¨ì–´
        "american-words.20",
        "american-words.35",
        "american-words.40",
        "american-words.50",
        "american-words.55",
        "american-words.60",
        "american-words.70",
        "american-words.80",
    ]
    
    for filename in word_files:
        filepath = SCOWL_PATH / filename
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    word = line.strip().lower()
                    # í•„í„°ë§: ì˜ë¬¸ìë§Œ, 3ê¸€ì ì´ìƒ, 15ê¸€ì ì´í•˜
                    if (word and 
                        word.isalpha() and 
                        3 <= len(word) <= 15 and
                        not word.endswith("'s")):
                        words.add(word)
        
        if len(words) >= max_words:
            break
    
    # ì •ë ¬ í›„ ë°˜í™˜
    return sorted(list(words))[:max_words]


async def fetch_definition(session: aiohttp.ClientSession, word: str) -> Optional[dict]:
    """ë¹„ë™ê¸°ë¡œ ë‹¨ì–´ ì •ì˜ ê°€ì ¸ì˜¤ê¸°"""
    try:
        async with session.get(f"{DICTIONARY_API}{word}", timeout=aiohttp.ClientTimeout(total=10)) as response:
            if response.status == 200:
                data = await response.json()
                if data and len(data) > 0:
                    entry = data[0]
                    meanings = entry.get("meanings", [])
                    if meanings:
                        first_meaning = meanings[0]
                        definitions = first_meaning.get("definitions", [])
                        if definitions:
                            first_def = definitions[0]
                            definition = first_def.get("definition", "")
                            example = first_def.get("example", "")
                            
                            # ì˜ˆë¬¸ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ê³³ì—ì„œ ì°¾ê¸°
                            if not example:
                                for m in meanings:
                                    for d in m.get("definitions", []):
                                        if d.get("example"):
                                            example = d.get("example")
                                            break
                                    if example:
                                        break
                            
                            if not example:
                                example = f"I need to understand the word {word}."
                            
                            return {
                                "word": word,
                                "partOfSpeech": first_meaning.get("partOfSpeech", "unknown"),
                                "definition": definition,
                                "example": example
                            }
    except Exception:
        pass
    return None


async def translate_text(session: aiohttp.ClientSession, text: str) -> str:
    """ë¹„ë™ê¸°ë¡œ í•œêµ­ì–´ ë²ˆì—­"""
    if not text:
        return ""
    try:
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        text = text[:500]
        params = {
            "q": text,
            "langpair": "en|ko"
        }
        async with session.get(MYMEMORY_API, params=params, timeout=aiohttp.ClientTimeout(total=10)) as response:
            if response.status == 200:
                data = await response.json()
                if data.get("responseStatus") == 200:
                    translated = data.get("responseData", {}).get("translatedText", "")
                    return translated
    except Exception:
        pass
    return text  # ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜


def get_level(word: str) -> str:
    """ë‹¨ì–´ ë‚œì´ë„ ì¶”ì •"""
    if len(word) <= 4:
        return "A1"
    elif len(word) <= 5:
        return "A2"
    elif len(word) <= 7:
        return "B1"
    elif len(word) <= 9:
        return "B2"
    else:
        return "C1"


async def process_word(session: aiohttp.ClientSession, word: str, word_id: int, semaphore: asyncio.Semaphore) -> Optional[dict]:
    """ë‹¨ì–´ í•˜ë‚˜ ì²˜ë¦¬ (ì •ì˜ + ë²ˆì—­)"""
    async with semaphore:
        # 1. ì •ì˜ ê°€ì ¸ì˜¤ê¸°
        word_data = await fetch_definition(session, word)
        if not word_data:
            return None
        
        # 2. í•œêµ­ì–´ ë²ˆì—­
        definition_kr = await translate_text(session, word_data['definition'])
        example_kr = await translate_text(session, word_data['example'])
        
        return {
            "id": word_id,
            "word": word_data['word'],
            "level": get_level(word),
            "partOfSpeech": word_data['partOfSpeech'],
            "definition": word_data['definition'],
            "definitionKr": definition_kr,
            "example": word_data['example'],
            "exampleKr": example_kr
        }


async def process_batch(words: list[str], start_id: int, existing_set: set) -> list[dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‹¨ì–´ë“¤ ë³‘ë ¬ ì²˜ë¦¬"""
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    results = []
    
    connector = aiohttp.TCPConnector(limit=CONCURRENT_REQUESTS, limit_per_host=CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        current_id = start_id
        
        for word in words:
            if word.lower() not in existing_set:
                tasks.append(process_word(session, word, current_id, semaphore))
                current_id += 1
        
        # ì§„í–‰ ìƒí™© í‘œì‹œí•˜ë©° ì²˜ë¦¬
        completed = 0
        total = len(tasks)
        
        for coro in asyncio.as_completed(tasks):
            result = await coro
            completed += 1
            if result:
                results.append(result)
            
            if completed % 50 == 0 or completed == total:
                print(f"  ì§„í–‰: {completed}/{total} ({len(results)}ê°œ ì„±ê³µ)")
    
    return results


def load_existing_words(filepath: Path) -> tuple[list, set, int]:
    """ê¸°ì¡´ words.json ë¡œë“œ"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            words = json.load(f)
            existing = {w['word'].lower() for w in words}
            max_id = max([w['id'] for w in words], default=0)
            return words, existing, max_id
    except FileNotFoundError:
        return [], set(), 0


def save_words(filepath: Path, words: list):
    """words.json ì €ì¥"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(words, f, ensure_ascii=False, indent=2)


async def main():
    print("=" * 60)
    print("ğŸš€ ì˜ì–´ ë‹¨ì–´ì¥ ëŒ€ëŸ‰ ë°ì´í„° ìƒì„± (ê³ ì† ë¹„ë™ê¸° ë²„ì „)")
    print("=" * 60)
    
    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    words_file = script_dir.parent / "assets" / "data" / "words.json"
    
    # ê¸°ì¡´ ë‹¨ì–´ ë¡œë“œ
    existing_words, existing_set, max_id = load_existing_words(words_file)
    print(f"ğŸ“š ê¸°ì¡´ ë‹¨ì–´ ìˆ˜: {len(existing_words)}")
    
    # ëª©í‘œ ë‹¨ì–´ ìˆ˜ ì…ë ¥
    try:
        target_count = int(input("ğŸ¯ ì¶”ê°€í•  ë‹¨ì–´ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ 1000): ") or "1000")
    except ValueError:
        target_count = 1000
    
    print(f"\nğŸ“– SCOWLì—ì„œ ë‹¨ì–´ ë¡œë“œ ì¤‘...")
    scowl_words = load_scowl_words(target_count + len(existing_set))
    
    # ì¤‘ë³µ ì œì™¸
    new_words = [w for w in scowl_words if w.lower() not in existing_set][:target_count]
    print(f"ğŸ†• ì¶”ê°€í•  ë‹¨ì–´: {len(new_words)}ê°œ")
    
    if not new_words:
        print("ì¶”ê°€í•  ìƒˆ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°°ì¹˜ ì²˜ë¦¬
    start_time = time.time()
    all_new_words = []
    
    for i in range(0, len(new_words), BATCH_SIZE):
        batch = new_words[i:i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        total_batches = (len(new_words) + BATCH_SIZE - 1) // BATCH_SIZE
        
        print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ)...")
        
        batch_results = await process_batch(
            batch,
            max_id + len(all_new_words) + 1,
            existing_set
        )
        
        all_new_words.extend(batch_results)
        
        # ì¤‘ê°„ ì €ì¥ (100ê°œë§ˆë‹¤)
        if len(all_new_words) % 100 == 0:
            temp_words = existing_words + all_new_words
            save_words(words_file, temp_words)
            print(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ (ì´ {len(temp_words)}ê°œ)")
    
    # ìµœì¢… ì €ì¥
    final_words = existing_words + all_new_words
    save_words(words_file, final_words)
    
    elapsed = time.time() - start_time
    
    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print(f"   - ì¶”ê°€ëœ ë‹¨ì–´: {len(all_new_words)}ê°œ")
    print(f"   - ì´ ë‹¨ì–´ ìˆ˜: {len(final_words)}ê°œ")
    print(f"   - ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    print(f"   - ì²˜ë¦¬ ì†ë„: {len(all_new_words)/elapsed:.1f} ë‹¨ì–´/ì´ˆ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
